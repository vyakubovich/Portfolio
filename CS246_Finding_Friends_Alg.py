# -*- coding: utf-8 -*-
"""CS246_HW1_P1

Automatically generated by Colaboratory.

Original file is located at

Set up Spark
"""

!pip install pyspark
!pip install -U -q PyDrive2
#the output 'xxx is not a symbolic link' will not affect your implementation or execution
#to fix 'xxx is not a symbolic link', you can uncomment the lines starting from !mv xxxx
#you may need to replace xxx.11 with the correct version if other errors come up after colab update
#to get the correct version, use !ls /usr/local/lib to find out
!mv /usr/local/lib/libtbbmalloc_proxy.so.2 /usr/local/lib/libtbbmalloc_proxy.so.2.backup
!mv /usr/local/lib/libtbbmalloc.so.2 /usr/local/lib/libtbbmalloc.so.2.backup
!mv /usr/local/lib/libtbbbind_2_5.so.3 /usr/local/lib/libtbbbind_2_5.so.3.backup
!mv /usr/local/lib/libtbb.so.12 /usr/local/lib/libtbb.so.12.backup
!mv /usr/local/lib/libtbbbind_2_0.so.3 /usr/local/lib/libtbbbind_2_0.so.3.backup
!mv /usr/local/lib/libtbbbind.so.3 /usr/local/lib/libtbbbind.so.3.backup
!ln -s /usr/local/lib/libtbbmalloc_proxy.so.2.11 /usr/local/lib/libtbbmalloc_proxy.so.2
!ln -s /usr/local/lib/libtbbmalloc.so.2.11 /usr/local/lib/libtbbmalloc.so.2
!ln -s /usr/local/lib/libtbbbind_2_5.so.3.11 /usr/local/lib/libtbbbind_2_5.so.3
!ln -s /usr/local/lib/libtbb.so.12.11 /usr/local/lib/libtbb.so.12
!ln -s /usr/local/lib/libtbbbind_2_0.so.3.11 /usr/local/lib/libtbbbind_2_0.so.3
!ln -s /usr/local/lib/libtbbbind.so.3.11 /usr/local/lib/libtbbbind.so.3
#If error related to the above execution occurs, you can try commenting out the above 12 lines under pip install PyDrive2 (not included)

# !sudo ldconfig
!apt install openjdk-8-jdk-headless -qq
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

"""Authenticate Google Drive"""

from pydrive2.auth import GoogleAuth
from pydrive2.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials

# Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

"""Import Packages and File"""

from pyspark.sql import *
from pyspark.sql.functions import *
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from pyspark import SparkContext
import pandas as pd
from collections import Counter
import itertools

# create the Spark Session
spark = SparkSession.builder.getOrCreate()

# create the Spark Context
sc = spark.sparkContext

"""Recommendations part"""

#Generate pairs for each user: a pair for the user and each friend and a pair for each of the friends ()
#pairs = list(combinations(items_with_support, 2))
def get_pairs(row):
  min = -99999 #Set this so that we never get a friend recommendation for a user that
  #Set this so that we never get a friend recommendation for a user that is already a friend (counting the friend pairs for other users (the ) for filter will never be >0, so gets filtered)
  #i.e 0 and 4 may be friend and firednd of 3, this way we filter then out
  user, current_friends = row.split('\t')
  current_friends = current_friends.split(',')
  user_friend_pairs = [((user, friend), min) for friend in current_friends]
  friend_pairs = [(pair, 1) for pair in itertools.permutations(current_friends, 2)]
  return user_friend_pairs + friend_pairs

#Use pairs function to get all potential pairs of friends (map step)
pairs_pass = sc.textFile('soc-LiveJournal1Adj.txt', 16).flatMap(get_pairs)
pairs_pass.take(2)

#Get counts (reduce step)
counts = pairs_pass.reduceByKey(lambda x, y: x + y)
counts.take(2)

#Drop those that are friends already and rearrange the keys so that each user is in front and the potential mutual is in a tuple for counting
filtered_out_currents = counts.filter(lambda x: x[1] > 0).map(lambda x: (x[0][0], (x[0][1], x[1])))
filtered_out_currents.take(2)

#Group by each user to then proceed to get top potential new mutuals/recommendations
grouped_by_user = filtered_out_currents.groupByKey()
#result_rdd4.mapValues(list).collectAsMap()

#For each user (firend_recs[0]), get a count of how many of the user's friends are freinds with this new, potential friend (aka get the top 10 recs now)
final_result = grouped_by_user.map(lambda friend_recs: (
    friend_recs[0],
    sorted(
        Counter(dict((friend, count) for friend, count in friend_recs[1])).items(),
        key=lambda x: (x[1], int(x[0])),
        reverse=False)[:10]
    )
)

final_result.take(2)

#Pull recs for the users listed in the questions and then generate a dataframe for easy viewing
recomendations = []

user_ids = [924, 8941, 8942, 9019, 9020, 9021, 9022, 9990, 9992, 9993]
user_ids = [str(x) for x in user_ids]

for user in user_ids:
  result = final_result.lookup(user)
  print('Result done for ', user)
  recs_list = sorted([int(result[0][x][0]) for x in range(len(result[0]))])
  recomendations.append(recs_list)

results_dict = {'User': user_ids, 'Recommendations': recomendations}

results = pd.DataFrame(results_dict)

pd.set_option('display.max_colwidth', -1)
results

